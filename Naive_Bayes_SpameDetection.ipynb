{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes for Spam Detection (Classification)\n",
        "\n",
        "In this code, we are solving the text classification problem where the goal is to categorize SMS messages as either \"ham\" (non-spam) or \"spam\". Specifically, the task involves using the multinomial Naive Bayes classifier to predict whether a given message is spam based on its content. To do this, we load a dataset containing labelled messages (as \"spam\" or \"ham\"). Then, we convert the raw text data (SMS messages) into a numerical representation (using a bag-of-words model). We then train the Naive Bayes classifier on this (now) numerical data to learn the relationship between word frequencies and the two classes (ham or spam).\n",
        "\n",
        "The core step before applying Naive Bayes, is the \"tokenization\" of the text data (using the aforementioned \"bag-of-words\" model). To do this, we utilize the `CountVectorizer` class from `sklearn.feature_extraction.text`, which converts raw text messages into a bag-of-words representation (which is one possible way of numerically representing text data for machine learning tasks). The `CountVectorizer` splits each message into individual words (tokens). This process involves removing punctuation, handling case sensitivity (lowercasing), and breaking the text into meaningful units such as words. Then, it constructs a vocabulary (i.e., a set of unique tokens) from the entire dataset. It looks at all the messages and identifies the unique words. After building the vocabulary, the `CountVectorizer` counts how often each word appears in each document (SMS message) in the dataset. This produces a document-term matrix (DTM), where:\n",
        "\n",
        "\n",
        "\n",
        "*   each row represents a message (or document),\n",
        "*   each column represents a word in the vocabulary,\n",
        "*   and  the cell value at position $(i, j)$ represents the (frequency) count of word $j$ in document $i$.\n",
        "\n",
        "That way, we create discrete features vectors, with components representing the count (frequency) of a specific work (token) from the vocabulary."
      ],
      "metadata": {
        "id": "oIOwEtRCtEr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4nqnOmbtBbt",
        "outputId": "7677bc53-34ec-4840-cc86-11c035cc7b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.98\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99       966\n",
            "        Spam       0.92      0.96      0.94       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.97      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "Custom Message Prediction:  Spam\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset (assumes dataset is downloaded locally or from an online source)\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "data = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Map labels to binary: \"ham\" -> 0, \"spam\" -> 1\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Extract features and labels\n",
        "X = data['message']\n",
        "y = data['label']\n",
        "\n",
        "# Convert text data to a bag-of-words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Naive Bayes classifier\n",
        "# Since we have discrete features, we utilize the multinomial model\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Test on a custom message\n",
        "custom_message = [\"Congratulations! You've won a free trip to Bahamas! Reply now to claim.\"]\n",
        "custom_vectorized = vectorizer.transform(custom_message)\n",
        "prediction = nb_classifier.predict(custom_vectorized)\n",
        "print(\"\\nCustom Message Prediction: \", \"Spam\" if prediction[0] == 1 else \"Ham\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results, we can observe that despite the strong (and unrealistic) naive Bayes assumption, we obtain a very good accuracy in classifying spam messages. Note that without the Naive Bayes assumption, the classification problem using the \"bag-of-words\" encoding would be significantly more computationally intensive, since we would need to model the dependence between each pair of features (we would need to consider all possible combinations of features, which leads to a combinatorial explosion in the number of model parameters)."
      ],
      "metadata": {
        "id": "-nTbu6Xzt3xT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erutkbpXtGSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}